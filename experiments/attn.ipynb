{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at some point I'll want to train my model. The data will be tokens where each token has a dimension $d_e$ (e for embedding).\n",
    "\n",
    "Given a particular window having a number of tokens as ($d_w$), \n",
    "\n",
    "for example a sentence: \"Hello my name is Donny\" -> is a tensor of shape $(d_w, d_e)$ and we probably want to sample many such sentences during training to have a single batch as $(d_b, d_w, d_e)$\n",
    "\n",
    "Batches complicate things and so does the dimensionality of the $d_e$, so I will ignore that for now.\n",
    "\n",
    "My goal is given a tensor of $(d_w, d_e)$ where $d_e=1$ to weigh each token (self-attention). So given $(d_w, d_e)$ I should get $(d_w, d_e)$ back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "d_w, d_e = 5,1\n",
    "x = torch.randn((d_e, d_w))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to model the dependencies between the embedding is to simply sum them up, then hope that information can be used to predict the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not a bad first strategy! So let's keep this one. What I then want to do is all at once predict the next token given the sum of previous token.\n",
    "\n",
    "So I need a way to grab those values, I think I can do that with a lower triangular ones matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5410, -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 1.5410, -0.2934, -0.0000,  0.0000, -0.0000],\n",
       "        [ 1.5410, -0.2934, -2.1788,  0.0000, -0.0000],\n",
       "        [ 1.5410, -0.2934, -2.1788,  0.5684, -0.0000],\n",
       "        [ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((d_seq_len, d_seq_len)))\n",
    "mask*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And summing across the rows I could have just matmuled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410],\n",
      "        [ 1.2476],\n",
      "        [-0.9312],\n",
      "        [-0.3628],\n",
      "        [-1.4473]])\n",
      "tensor([[ 1.5410],\n",
      "        [ 1.2476],\n",
      "        [-0.9312],\n",
      "        [-0.3628],\n",
      "        [-1.4473]])\n",
      "tensor([[ 1.5410,  1.2476, -0.9312, -0.3628, -1.4473]])\n"
     ]
    }
   ],
   "source": [
    "print((mask*x).sum(-1, keepdim=True))\n",
    "print(mask@x.T)\n",
    "print(x@torch.triu(torch.ones((d_w, d_w)))) # no need for transpose!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want to do an average (equal weighting to every value to predict the next).\n",
    "\n",
    "This is just a sum divided by the number of elements per. So for the first one is 1. The second should have 0.5, 0.5, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5000, 0.3333, 0.2500, 0.2000],\n",
       "        [0.0000, 0.5000, 0.3333, 0.2500, 0.2000],\n",
       "        [0.0000, 0.0000, 0.3333, 0.2500, 0.2000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2500, 0.2000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mask = torch.triu(torch.ones((d_w, d_w)))\n",
    "w_mask  /= w_mask.sum(0, keepdim=True)\n",
    "w_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410],\n",
      "        [ 0.6238],\n",
      "        [-0.3104],\n",
      "        [-0.0907],\n",
      "        [-0.2895]])\n",
      "tensor([[ 1.5410,  0.6238, -0.3104, -0.0907, -0.2895]])\n"
     ]
    }
   ],
   "source": [
    "print(mask@x.T / mask.sum(-1, keepdim=True))\n",
    "print(x@w_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by changing the the weights, I can do other things than average. I can weigh the first token higher (or whatever)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's compute that weight matrix on the fly. \n",
    "\n",
    "The Attention is all you need paper does \n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "So the softmax portion is what computes the weighted sum basically and the V is essentially the x (although projected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [-inf, 1., 1., 1., 1.],\n",
       "        [-inf, -inf, 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, 1.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones((d_w, d_w)))\n",
    "mask.masked_fill_(mask == 0, float(\"-inf\"))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5000, 0.3333, 0.2500, 0.2000],\n",
       "        [0.0000, 0.5000, 0.3333, 0.2500, 0.2000],\n",
       "        [0.0000, 0.0000, 0.3333, 0.2500, 0.2000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2500, 0.2000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2000]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(mask, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need a method to dynamically generate form learned weights that matrix!\n",
    "\n",
    "Here comes in Q, K in the attention compution. These compute attention values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 3\n",
    "Q = torch.randn((d_w, d_k))\n",
    "K = torch.randn((d_w, d_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4948,  0.0305,  0.8477,  0.0659, -1.1022],\n",
       "        [-0.8167, -0.8018,  0.7273,  0.6297,  0.5582],\n",
       "        [-0.0843,  0.7896,  0.2424, -1.1661, -1.6582],\n",
       "        [-0.4332, -0.8788,  0.7489,  0.9102,  0.6452],\n",
       "        [-1.4891, -1.0330,  0.1476,  0.7217,  1.6950]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First find similarities within the input\n",
    "# (QK^T)/sqrt(d_k)\n",
    "inside = Q@K.T*(d_k**(-.5)) # (d_w, d_w)\n",
    "inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But really I only want to model similarities between the masked values, so here I used the masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6968, 0.4111, 0.1860, 0.0345],\n",
       "        [0.0000, 0.3032, 0.3645, 0.3269, 0.1816],\n",
       "        [0.0000, 0.0000, 0.2244, 0.0543, 0.0198],\n",
       "        [0.0000, 0.0000, 0.0000, 0.4328, 0.1981],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5660]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(inside)\n",
    "mask.masked_fill_(mask == 0, float(\"-inf\"))\n",
    "qk = torch.softmax(mask, dim=0)\n",
    "qk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = torch.randn((d_w, d_k))\n",
    "qkv = qk@V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
