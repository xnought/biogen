{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['doi', 'title', 'authors', 'author_corresponding', 'author_corresponding_institution', 'date', 'version', 'type', 'license', 'category', 'jatsxml', 'abstract', 'published', 'server'],\n",
      "        num_rows: 353648\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"laion/biorXiv_metadata\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'title', 'authors', 'author_corresponding', 'author_corresponding_institution', 'date', 'version', 'type', 'license', 'category', 'jatsxml', 'abstract', 'published', 'server'],\n",
       "    num_rows: 353648\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = ds[\"train\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize(x: str):\n",
    "\tx = re.sub(r'\\s', ' ', x)\n",
    "\tx = x.lower()\n",
    "\n",
    "\tallowed_chars = set(list(\" abcdefghijklmnopqrstuvwxyz?<>!.,:;()-+=&$*%^@1234567890/–'\\\"\"))\n",
    "\tresult = \"\"\n",
    "\tfor c in x:\n",
    "\t\tif c in allowed_chars:\n",
    "\t\t\tresult += c\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Population genomics of Saccharomyces cerevisiae human isolates: passengers, colonizers, invaders.', 'Estimating seed bank accumulation and dynamics in three obligate-seeder Proteaceae species', 'How and where to look for tRNAs in Metazoan mitochondrial genomes, and what you might find when you get there', 'How and where to look for tRNAs in Metazoan mitochondrial genomes, and what you might find when you get there', 'Tracking global changes induced in the CD4 T cell receptor repertoire by immunization with a complex antigen using short stretches of CDR3 protein sequence.', 'The shrinking human protein coding complement: are there fewer than 20,000 genes?', 'Emergence of structural and dynamical properties of ecological mutualistic networks', 'Expertly validated models suggest responses to climate change are related to species traits: a phylogenetically-controlled analysis of the Order Lagomorpha', 'Expertly validated models suggest responses to climate change are related to species traits: a phylogenetically-controlled analysis of the Order Lagomorpha', 'The\\xa0emergence\\xa0of\\xa0the\\xa0rescue\\xa0effect\\xa0from\\xa0explicit\\xa0within- and\\xa0between-patch\\xa0dynamics\\xa0in\\xa0a metapopulation']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['population genomics of saccharomyces cerevisiae human isolates: passengers, colonizers, invaders.',\n",
       " 'estimating seed bank accumulation and dynamics in three obligate-seeder proteaceae species',\n",
       " 'how and where to look for trnas in metazoan mitochondrial genomes, and what you might find when you get there',\n",
       " 'how and where to look for trnas in metazoan mitochondrial genomes, and what you might find when you get there',\n",
       " 'tracking global changes induced in the cd4 t cell receptor repertoire by immunization with a complex antigen using short stretches of cdr3 protein sequence.',\n",
       " 'the shrinking human protein coding complement: are there fewer than 20,000 genes?',\n",
       " 'emergence of structural and dynamical properties of ecological mutualistic networks',\n",
       " 'expertly validated models suggest responses to climate change are related to species traits: a phylogenetically-controlled analysis of the order lagomorpha',\n",
       " 'expertly validated models suggest responses to climate change are related to species traits: a phylogenetically-controlled analysis of the order lagomorpha',\n",
       " 'the emergence of the rescue effect from explicit within- and between-patch dynamics in a metapopulation']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train[:10][\"title\"])\n",
    "[normalize(x) for x in train[:10][\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353648/353648 [00:49<00:00, 7141.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "unique_chars = set()\n",
    "for t in tqdm(train):\n",
    "\tfor c in t[\"title\"]:\n",
    "\t\tunique_chars.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\xa0',\n",
       " '®',\n",
       " '°',\n",
       " '´',\n",
       " '·',\n",
       " 'Å',\n",
       " 'Ô',\n",
       " '×',\n",
       " 'á',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'í',\n",
       " 'ï',\n",
       " 'ô',\n",
       " 'ö',\n",
       " 'ü',\n",
       " 'ı',\n",
       " 'ĸ',\n",
       " 'Ō',\n",
       " 'ɛ',\n",
       " 'ʟ',\n",
       " 'ʹ',\n",
       " '̈',\n",
       " 'Β',\n",
       " 'Γ',\n",
       " 'Δ',\n",
       " 'Ρ',\n",
       " 'Τ',\n",
       " 'Φ',\n",
       " 'α',\n",
       " 'β',\n",
       " 'γ',\n",
       " 'δ',\n",
       " 'ε',\n",
       " 'ζ',\n",
       " 'η',\n",
       " 'θ',\n",
       " 'κ',\n",
       " 'μ',\n",
       " 'π',\n",
       " 'σ',\n",
       " 'φ',\n",
       " 'χ',\n",
       " 'ω',\n",
       " 'ϵ',\n",
       " 'қ',\n",
       " '\\u200b',\n",
       " '\\u200e',\n",
       " '‐',\n",
       " '‑',\n",
       " '–',\n",
       " '—',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '\\u2029',\n",
       " '′',\n",
       " '″',\n",
       " '℃',\n",
       " '™',\n",
       " '→',\n",
       " '∆',\n",
       " '−',\n",
       " '≥',\n",
       " '：',\n",
       " '￼'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 10, 7, 14, 14, 17, 2, 22, 10, 7, 20, 7, 30, 1], [0, 25, 10, 3, 22, 2, 11, 21, 2, 23, 18, 30, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>hello there!<e>', '<s>what is up!<e>']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer_encode(strings: list[str], alphabet: str):\n",
    "\tindex_map = {\n",
    "\t\t\"<s>\": 0,\n",
    "\t\t\"<e>\": 1,\n",
    "\t}\n",
    "\tfor i in range(len(alphabet)):\n",
    "\t\tindex_map[alphabet[i]] = i+2\n",
    "\t\n",
    "\tres = []\n",
    "\tfor s in strings:\n",
    "\t\tsub = [0] # <s>\n",
    "\t\tfor c in s:\n",
    "\t\t\tsub.append(index_map[c])\n",
    "\t\tsub.append(1) # <e>\n",
    "\t\tres.append(sub)\n",
    "\treturn res\n",
    "\n",
    "def tokenizer_decode(idxs: list[list[int]], alphabet: str):\n",
    "\treverse_index_map = [\"<s>\", \"<e>\"]\n",
    "\treverse_index_map.extend(alphabet)\n",
    "\n",
    "\n",
    "\tres = []\n",
    "\tfor sub in idxs:\n",
    "\t\tstring = \"\"\n",
    "\t\tfor i in sub:\n",
    "\t\t\tstring += reverse_index_map[i]\n",
    "\t\tres.append(string)\n",
    "\treturn res\n",
    "\n",
    "alphabet=[' ','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','?','!','.',',',':',';','&','%','1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "enc = tokenizer_encode([\"hello there!\", \"what is up!\"], alphabet)\n",
    "print(enc)\n",
    "tokenizer_decode(enc, alphabet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
